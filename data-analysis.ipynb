{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文情感分析\n",
    "\n",
    "- 数据分析\n",
    "- 预处理\n",
    "- 词向量\n",
    "- TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0-rc0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chinese_stopwords.txt', 'neg.txt', 'pos.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/pos.txt', 'r') as f:\n",
    "    pos_text = f.readlines()\n",
    "with open('./data/neg.txt', 'r') as f:\n",
    "    neg_text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['质量好,做工也不错,尺码标准,\\n',\n",
       " '裤子质量很好，裤型不错，而且穿起来显瘦，性比价高，是我喜欢的布料，不起球，值得购买，\\n',\n",
       " '做工很好，货真价实，质量不错满意！\\n',\n",
       " '真是不错呢！裤子质量很好，穿着也很有型，帅帅的，更喜欢他了呢！快递很快，态度也特别好，跟客服咨询问题能够得到快快速的回答，下次还会光顾，么么哒\\n',\n",
       " '品质还可以，就是有点掉色，可以接受\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['穿上不舒服，颜色和质感跟图片差异很大，建议慎重购买！后悔了！\\n',\n",
       " '真心垃圾，以后再也不会买班尼路了\\n',\n",
       " '物流超慢，整整6天多，准备退订了才到，无语。衣服质量嘛，中下水准，穿着不太舒服。衣服穿起来的外形跟农民工差不多，设计太垃圾了。总之不值这个价，三分之一的价格买来都觉得亏。谁买谁哭！\\n',\n",
       " '这个T恤不安逸得，和农贸市场一二十的差不多样\\n',\n",
       " '你以为是Lee那就错了，你以为会有Lee一半的品质，那就又错了，你以为会送货很快，那就又错了！\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/wg/fjh1kvtj1v5g555zy343tkkm0000gn/T/jieba.cache\n",
      "Loading model cost 0.713 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "pos_segs = [\" \".join(jieba.cut(x)) for x in pos_text]\n",
    "neg_segs = [\" \".join(jieba.cut(x)) for x in neg_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['质量 好 , 做工 也 不错 , 尺码 标准 , \\n',\n",
       " '裤子 质量 很 好 ， 裤型 不错 ， 而且 穿 起来 显瘦 ， 性 比价 高 ， 是 我 喜欢 的 布料 ， 不 起球 ， 值得 购买 ， \\n',\n",
       " '做工 很 好 ， 货真价实 ， 质量 不错 满意 ！ \\n',\n",
       " '真是 不错 呢 ！ 裤子 质量 很 好 ， 穿着 也 很 有 型 ， 帅帅 的 ， 更 喜欢 他 了 呢 ！ 快递 很快 ， 态度 也 特别 好 ， 跟 客服 咨询 问题 能够 得到 快 快速 的 回答 ， 下次 还会 光顾 ， 么 么 哒 \\n',\n",
       " '品质 还 可以 ， 就是 有点 掉色 ， 可以 接受 \\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_segs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['穿 上 不 舒服 ， 颜色 和 质感 跟 图片 差异 很大 ， 建议 慎重 购买 ！ 后悔 了 ！ \\n',\n",
       " '真心 垃圾 ， 以后 再也不会 买 班尼路 了 \\n',\n",
       " '物流 超慢 ， 整整 6 天多 ， 准备 退订 了 才 到 ， 无 语 。 衣服 质量 嘛 ， 中 下 水准 ， 穿着 不太 舒服 。 衣服 穿 起来 的 外形 跟 农民工 差不多 ， 设计 太 垃圾 了 。 总之 不值 这个 价 ， 三分之一 的 价格 买来 都 觉得 亏 。 谁 买 谁 哭 ！ \\n',\n",
       " '这个 T恤 不 安逸 得 ， 和 农贸市场 一二十 的 差不多 样 \\n',\n",
       " '你 以为 是 Lee 那 就 错 了 ， 你 以为 会 有 Lee 一半 的 品质 ， 那 就 又 错 了 ， 你 以为 会 送货 很快 ， 那 就 又 错 了 ！ \\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_segs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- POSITIVE TEXT --------------------\n",
      "Total number: 25000\n",
      "Average length: 32.187\n",
      "Max length: 804\n",
      "Min length: 2\n",
      "Most common words : \n",
      "[('，', 84928), ('的', 37743), ('。', 25705), ('很', 19492), ('了', 16266), ('好', 13058), ('不错', 11836), ('！', 10658), ('也', 8951), ('是', 8485), (',', 7216), ('还', 6902), ('酒店', 5990), ('买', 5980), ('我', 5585), ('在', 4998), ('都', 4706), ('有', 4503), ('就', 4219), ('用', 4105), ('京东', 4070), ('房间', 4015), ('非常', 3894), ('可以', 3800), ('不', 3542), ('.', 3366), ('质量', 3165), ('没有', 3140), ('就是', 3108), ('服务', 2962), ('感觉', 2866), ('和', 2796), ('苹果', 2671), ('比较', 2664), ('喜欢', 2655), ('到', 2575), ('给', 2569), ('还是', 2538), ('挺', 2378), ('满意', 2204), ('这个', 2153), ('快', 2139), ('价格', 2118), ('、', 2040), ('裤子', 2005), ('舒服', 1887), ('住', 1867), ('上', 1833), ('物流', 1829), ('大', 1801), ('快递', 1796), ('收到', 1768), ('一个', 1760), ('方便', 1734), ('东西', 1707), ('吃', 1694), ('去', 1673), ('速度', 1671), ('但', 1655), ('下次', 1640), ('没', 1606), ('包装', 1586), ('一直', 1558), ('说', 1553), ('穿', 1529), ('我们', 1524), ('入住', 1513), ('有点', 1509), ('?', 1483), ('很快', 1453), ('比', 1434), ('小', 1427), ('不过', 1399), ('来', 1398), ('吧', 1377), ('再', 1372), ('购买', 1369), ('但是', 1358), ('早餐', 1354), ('值得', 1318), ('味道', 1296), ('要', 1279), ('：', 1271), ('特别', 1239), ('这', 1234), ('又', 1220), ('多', 1202), ('会', 1169), ('而且', 1137), ('合适', 1136), ('看', 1130), ('便宜', 1124), ('好吃', 1063), ('真的', 1057), ('实惠', 1045), ('大小', 1038), ('不是', 1024), ('好评', 1021), ('着', 1017), ('人', 1014)] \n",
      "\n",
      "-------------------- NEGATIVE TEXT --------------------\n",
      "Total number: 25000\n",
      "Average length: 33.97808\n",
      "Max length: 1280\n",
      "Min length: 1\n",
      "Most common words : \n",
      "[('，', 80336), ('的', 38889), ('了', 26767), ('。', 22704), ('！', 16133), ('是', 11898), ('我', 10912), ('不', 10240), ('都', 8369), ('就', 7294), ('买', 7099), ('也', 6666), ('没有', 5640), ('还', 5509), ('酒店', 5202), ('在', 4975), ('很', 4931), (',', 4775), ('有', 4757), ('说', 4655), ('京东', 4295), ('？', 4054), ('给', 3983), ('房间', 3962), ('用', 3177), ('差', 3088), ('没', 3054), ('好', 2889), ('一个', 2841), ('这', 2768), ('到', 2705), ('和', 2552), ('太', 2500), ('就是', 2288), ('不好', 2249), ('这个', 2235), ('.', 2155), ('差评', 2152), ('苹果', 2117), ('什么', 2102), ('东西', 2053), ('要', 1973), ('去', 1944), ('服务', 1939), ('不是', 1904), ('还是', 1899), ('感觉', 1899), ('*', 1895), ('垃圾', 1873), ('又', 1850), ('你', 1821), ('客服', 1807), ('小', 1805), ('住', 1796), ('个', 1782), ('才', 1774), ('人', 1734), ('…', 1698), ('、', 1681), ('可以', 1658), ('知道', 1636), ('上', 1586), ('而且', 1575), ('吧', 1545), ('一样', 1527), ('非常', 1521), ('让', 1513), ('一般', 1484), ('这样', 1463), ('质量', 1441), ('啊', 1433), ('前台', 1431), ('手机', 1384), ('入住', 1381), ('再', 1350), ('失望', 1319), ('多', 1308), ('大', 1307), ('一次', 1304), ('问题', 1296), ('不能', 1287), ('但是', 1282), ('不会', 1268), ('我们', 1265), ('过', 1258), ('以后', 1234), ('快递', 1228), ('吗', 1223), ('跟', 1204), ('价格', 1188), ('后', 1186), ('吃', 1174), ('时候', 1171), ('真的', 1165), ('结果', 1160), ('不要', 1157), ('还有', 1141), ('这么', 1130), ('第一次', 1127), ('坏', 1121)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-' * 20 + ' POSITIVE TEXT ' + '-' * 20)\n",
    "print(\"Total number: {}\".format(len(pos_segs)))\n",
    "print(\"Average length: {}\".format(np.mean([len(sentence.split()) for sentence in pos_segs])))\n",
    "print(\"Max length: {}\".format(np.max([len(sentence.split()) for sentence in pos_segs])))\n",
    "print(\"Min length: {}\".format(np.min([len(sentence.split()) for sentence in pos_segs])))\n",
    "pos_text_seg = \" \".join(pos_segs)\n",
    "c = Counter(pos_text_seg.split()).most_common(100)\n",
    "print(\"Most common words : \\n{} \\n\".format(c))\n",
    "\n",
    "print(\"-\" * 20 + \" NEGATIVE TEXT \" + \"-\" * 20)\n",
    "print(\"Total number: {}\".format(len(neg_segs)))\n",
    "print(\"Average length: {}\".format(np.mean([len(sentence.split()) for sentence in neg_segs])))\n",
    "print(\"Max length: {}\".format(np.max([len(sentence.split()) for sentence in neg_segs])))\n",
    "print(\"Min length: {}\".format(np.min([len(sentence.split()) for sentence in neg_segs])))\n",
    "neg_text_seg = \" \".join(neg_segs)\n",
    "c = Counter(neg_text_seg.split()).most_common(100)\n",
    "print(\"Most common words : \\n{} \\n\".format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从中可以看出高频词中有很多是无意义的停用词，因此我们进一步对数据进行去停用词操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['质量 做工 不错 尺码 标准',\n",
       " '裤子 质量 裤型 不错 穿 起来 显瘦 性 比价 高 喜欢 布料 起球 值得 购买',\n",
       " '做工 货真价实 质量 不错 满意',\n",
       " '真是 不错 裤子 质量 穿着 型 帅帅 更 喜欢 快递 很快 态度 特别 客服 咨询 问题 能够 得到 快 快速 回答 下次 还会 光顾 哒',\n",
       " '品质 有点 掉色 接受',\n",
       " '质量 不错 尺码 大小 合适 喜欢',\n",
       " '喜欢 满意 下次 会 不会 掉色 质量 不错',\n",
       " '质量 不错 价格 贵 穿 裤型 快递 小哥 超级 赞',\n",
       " '裤子 没有 色差 尺码 正好 弹力 真心 不错',\n",
       " '裤子 真心 不错 店家 态度 还会 光顾']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_path = '/Users/huih/pinduoduo/stopwords/chinese_stopwords.txt'\n",
    "\n",
    "\n",
    "# 创建停用词列表\n",
    "def get_stop_words():\n",
    "    stopwords = [line.strip() for line in open(stopword_path, encoding='UTF-8').readlines()]\n",
    "    stopwords += [\",\", \"'\"]\n",
    "    return stopwords\n",
    "stopwords = get_stop_words()\n",
    "\n",
    "pos_segs_without_stop = []\n",
    "for sentence in pos_segs:\n",
    "    sentWords = [x.strip() for x in sentence.split(' ') if x.strip() and x.strip() not in stopwords]\n",
    "    pos_segs_without_stop.append(' '.join(sentWords))\n",
    "\n",
    "pos_segs_without_stop[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['穿 舒服 颜色 质感 图片 差异 很大 建议 慎重 购买 后悔',\n",
       " '真心 垃圾 以后 再也不会 买 班尼路',\n",
       " '物流 超慢 整整 天多 准备 退订 语 衣服 质量 中 水准 穿着 不太 舒服 衣服 穿 起来 外形 农民工 差不多 设计 太 垃圾 不值 价 三分之一 价格 买来 觉得 亏 买 哭',\n",
       " 'T恤 安逸 农贸市场 一二十 差不多 样',\n",
       " 'Lee 错 会 Lee 一半 品质 错 会 送货 很快 错',\n",
       " '语 - 裙子',\n",
       " '发货 慢 10 天才 收到 质量 差 线头 松动 第二天 起毛 建议 不要 买 过来人 他家 忠告',\n",
       " '只能 说 质量 很差 肯定 正品',\n",
       " '图片',\n",
       " '差评 半 蓝色 发 黑色 32 - 发 31']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_segs_without_stop = []\n",
    "for sentence in neg_segs:\n",
    "    sentWords = [x.strip() for x in sentence.split(' ') if x.strip() and x.strip() not in stopwords]\n",
    "    neg_segs_without_stop.append(' '.join(sentWords))\n",
    "\n",
    "neg_segs_without_stop[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- POSITIVE TEXT --------------------\n",
      "Total number: 25000\n",
      "Average length: 17.1128\n",
      "Max length: 427\n",
      "Min length: 0\n",
      "Most common words : \n",
      "[('不错', 11836), ('酒店', 5990), ('买', 5980), ('京东', 4070), ('房间', 4015), ('非常', 3894), ('.', 3366), ('质量', 3165), ('没有', 3140), ('服务', 2962), ('感觉', 2866), ('苹果', 2671), ('比较', 2664), ('喜欢', 2655), ('挺', 2378), ('满意', 2204), ('快', 2139), ('价格', 2118), ('裤子', 2005), ('舒服', 1887), ('住', 1867), ('物流', 1829), ('快递', 1796), ('收到', 1768), ('一个', 1760), ('方便', 1734), ('东西', 1707), ('吃', 1694), ('速度', 1671), ('下次', 1640), ('没', 1606), ('包装', 1586), ('一直', 1558), ('说', 1553), ('穿', 1529), ('入住', 1513), ('有点', 1509), ('很快', 1453), ('购买', 1369), ('早餐', 1354), ('值得', 1318), ('味道', 1296), ('特别', 1239), ('会', 1169), ('合适', 1136), ('便宜', 1124), ('好吃', 1063), ('真的', 1057), ('实惠', 1045), ('大小', 1038), ('好评', 1021), ('活动', 990), ('华为', 957), ('平板', 943), ('起来', 939), ('高', 935), ('性价比', 927), ('新鲜', 926), ('购物', 924), ('设施', 910), ('环境', 901), ('以后', 891), ('一次', 885), ('知道', 873), ('!', 868), ('…', 864), ('送', 862), ('还会', 860), ('超市', 860), ('希望', 852), ('效果', 834), ('月', 827), ('应该', 820), ('宾馆', 820), ('送货', 819), ('干净', 819), ('很多', 816), ('支持', 809), ('总体', 794), ('问题', 786), ('推荐', 769), ('点', 766), ('前台', 764), ('评价', 761), ('穿着', 760), ('年', 756), ('水果', 755), ('已经', 754), ('晚上', 744), ('衣服', 742), ('）', 738), ('划算', 712), ('服务态度', 699), ('日', 699), ('~', 692), ('朋友', 692), ('一点', 685), ('元', 677), ('正品', 674), ('（', 674)] \n",
      "\n",
      "-------------------- NEGATIVE TEXT --------------------\n",
      "Total number: 25000\n",
      "Average length: 17.74668\n",
      "Max length: 607\n",
      "Min length: 0\n",
      "Most common words : \n",
      "[('买', 7099), ('没有', 5640), ('酒店', 5202), ('说', 4655), ('京东', 4295), ('房间', 3962), ('差', 3088), ('没', 3054), ('一个', 2841), ('太', 2500), ('不好', 2249), ('.', 2155), ('差评', 2152), ('苹果', 2117), ('东西', 2053), ('服务', 1939), ('感觉', 1899), ('*', 1895), ('垃圾', 1873), ('客服', 1807), ('住', 1796), ('…', 1698), ('知道', 1636), ('非常', 1521), ('质量', 1441), ('前台', 1431), ('手机', 1384), ('入住', 1381), ('失望', 1319), ('一次', 1304), ('问题', 1296), ('不能', 1287), ('不会', 1268), ('以后', 1234), ('快递', 1228), ('价格', 1188), ('吃', 1174), ('真的', 1165), ('不要', 1157), ('第一次', 1127), ('坏', 1121), ('一点', 1105), ('烂', 1102), ('收到', 1080), ('送', 1076), ('裤子', 1024), ('不错', 992), ('味道', 982), ('携程', 967), ('服务员', 916), ('包装', 909), ('穿', 901), ('水果', 895), ('想', 890), ('洗', 881), ('假货', 866), ('满意', 864), ('比较', 856), ('设施', 839), ('现在', 834), ('居然', 833), ('里', 827), ('!', 817), ('货', 816), ('根本', 815), ('会', 813), ('有点', 799), ('晚上', 795), ('特别', 792), ('早餐', 787), ('月', 786), ('已经', 785), ('发现', 751), ('问', 748), ('衣服', 745), ('态度', 744), ('换', 743), ('里面', 743), ('华为', 742), ('购物', 739), ('找', 734), ('元', 733), ('）', 720), ('一直', 718), ('真是', 712), ('时', 699), ('两个', 697), ('退货', 696), ('头发', 690), ('（', 688), ('物流', 687), ('只能', 678), ('不想', 677), ('点', 675), ('不到', 672), ('号', 669), ('打开', 665), ('很多', 655), ('很差', 651), ('洗发水', 650)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 对去停用词后对数据进行统计分析\n",
    "pos_segs = pos_segs_without_stop\n",
    "neg_segs = neg_segs_without_stop\n",
    "\n",
    "print('-' * 20 + ' POSITIVE TEXT ' + '-' * 20)\n",
    "print(\"Total number: {}\".format(len(pos_segs)))\n",
    "print(\"Average length: {}\".format(np.mean([len(sentence.split()) for sentence in pos_segs])))\n",
    "print(\"Max length: {}\".format(np.max([len(sentence.split()) for sentence in pos_segs])))\n",
    "print(\"Min length: {}\".format(np.min([len(sentence.split()) for sentence in pos_segs])))\n",
    "pos_text_seg = \" \".join(pos_segs)\n",
    "c = Counter(pos_text_seg.split()).most_common(100)\n",
    "print(\"Most common words : \\n{} \\n\".format(c))\n",
    "\n",
    "print(\"-\" * 20 + \" NEGATIVE TEXT \" + \"-\" * 20)\n",
    "print(\"Total number: {}\".format(len(neg_segs)))\n",
    "print(\"Average length: {}\".format(np.mean([len(sentence.split()) for sentence in neg_segs])))\n",
    "print(\"Max length: {}\".format(np.max([len(sentence.split()) for sentence in neg_segs])))\n",
    "print(\"Min length: {}\".format(np.min([len(sentence.split()) for sentence in neg_segs])))\n",
    "neg_text_seg = \" \".join(neg_segs)\n",
    "c = Counter(neg_text_seg.split()).most_common(100)\n",
    "print(\"Most common words : \\n{} \\n\".format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上可以看出，去停用词后句子的平均长度由32和34变成了17，最大长度由804和1280变成了427和607(说明差评的平均长度要长于好评)。\n",
    "最小长度有2和1变成了0，说明数据中有些无效样本可以进一步删除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length for pos text: 1\n",
      "Min length for neg text: 1\n"
     ]
    }
   ],
   "source": [
    "while '' in pos_segs:\n",
    "    pos_segs.remove('')\n",
    "while '' in neg_segs:\n",
    "    neg_segs.remove('')\n",
    "print(\"Min length for pos text: {}\".format(np.min([len(sentence.split()) for sentence in pos_segs])))\n",
    "print(\"Min length for neg text: {}\".format(np.min([len(sentence.split()) for sentence in neg_segs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- POSITIVE TEXT --------------------\n",
      "Total number: 24967\n",
      "Average length: 17.135418752753633\n",
      "Max length: 427\n",
      "Min length: 1\n",
      "Most common words : \n",
      "[('不错', 11836), ('酒店', 5990), ('买', 5980), ('京东', 4070), ('房间', 4015), ('非常', 3894), ('.', 3366), ('质量', 3165), ('没有', 3140), ('服务', 2962), ('感觉', 2866), ('苹果', 2671), ('比较', 2664), ('喜欢', 2655), ('挺', 2378), ('满意', 2204), ('快', 2139), ('价格', 2118), ('裤子', 2005), ('舒服', 1887), ('住', 1867), ('物流', 1829), ('快递', 1796), ('收到', 1768), ('一个', 1760), ('方便', 1734), ('东西', 1707), ('吃', 1694), ('速度', 1671), ('下次', 1640), ('没', 1606), ('包装', 1586), ('一直', 1558), ('说', 1553), ('穿', 1529), ('入住', 1513), ('有点', 1509), ('很快', 1453), ('购买', 1369), ('早餐', 1354), ('值得', 1318), ('味道', 1296), ('特别', 1239), ('会', 1169), ('合适', 1136), ('便宜', 1124), ('好吃', 1063), ('真的', 1057), ('实惠', 1045), ('大小', 1038), ('好评', 1021), ('活动', 990), ('华为', 957), ('平板', 943), ('起来', 939), ('高', 935), ('性价比', 927), ('新鲜', 926), ('购物', 924), ('设施', 910), ('环境', 901), ('以后', 891), ('一次', 885), ('知道', 873), ('!', 868), ('…', 864), ('送', 862), ('还会', 860), ('超市', 860), ('希望', 852), ('效果', 834), ('月', 827), ('应该', 820), ('宾馆', 820), ('送货', 819), ('干净', 819), ('很多', 816), ('支持', 809), ('总体', 794), ('问题', 786), ('推荐', 769), ('点', 766), ('前台', 764), ('评价', 761), ('穿着', 760), ('年', 756), ('水果', 755), ('已经', 754), ('晚上', 744), ('衣服', 742), ('）', 738), ('划算', 712), ('服务态度', 699), ('日', 699), ('~', 692), ('朋友', 692), ('一点', 685), ('元', 677), ('正品', 674), ('（', 674)] \n",
      "\n",
      "-------------------- NEGATIVE TEXT --------------------\n",
      "Total number: 24964\n",
      "Average length: 17.77227207178337\n",
      "Max length: 607\n",
      "Min length: 1\n",
      "Most common words : \n",
      "[('买', 7099), ('没有', 5640), ('酒店', 5202), ('说', 4655), ('京东', 4295), ('房间', 3962), ('差', 3088), ('没', 3054), ('一个', 2841), ('太', 2500), ('不好', 2249), ('.', 2155), ('差评', 2152), ('苹果', 2117), ('东西', 2053), ('服务', 1939), ('感觉', 1899), ('*', 1895), ('垃圾', 1873), ('客服', 1807), ('住', 1796), ('…', 1698), ('知道', 1636), ('非常', 1521), ('质量', 1441), ('前台', 1431), ('手机', 1384), ('入住', 1381), ('失望', 1319), ('一次', 1304), ('问题', 1296), ('不能', 1287), ('不会', 1268), ('以后', 1234), ('快递', 1228), ('价格', 1188), ('吃', 1174), ('真的', 1165), ('不要', 1157), ('第一次', 1127), ('坏', 1121), ('一点', 1105), ('烂', 1102), ('收到', 1080), ('送', 1076), ('裤子', 1024), ('不错', 992), ('味道', 982), ('携程', 967), ('服务员', 916), ('包装', 909), ('穿', 901), ('水果', 895), ('想', 890), ('洗', 881), ('假货', 866), ('满意', 864), ('比较', 856), ('设施', 839), ('现在', 834), ('居然', 833), ('里', 827), ('!', 817), ('货', 816), ('根本', 815), ('会', 813), ('有点', 799), ('晚上', 795), ('特别', 792), ('早餐', 787), ('月', 786), ('已经', 785), ('发现', 751), ('问', 748), ('衣服', 745), ('态度', 744), ('换', 743), ('里面', 743), ('华为', 742), ('购物', 739), ('找', 734), ('元', 733), ('）', 720), ('一直', 718), ('真是', 712), ('时', 699), ('两个', 697), ('退货', 696), ('头发', 690), ('（', 688), ('物流', 687), ('只能', 678), ('不想', 677), ('点', 675), ('不到', 672), ('号', 669), ('打开', 665), ('很多', 655), ('很差', 651), ('洗发水', 650)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-' * 20 + ' POSITIVE TEXT ' + '-' * 20)\n",
    "print(\"Total number: {}\".format(len(pos_segs)))\n",
    "print(\"Average length: {}\".format(np.mean([len(sentence.split()) for sentence in pos_segs])))\n",
    "print(\"Max length: {}\".format(np.max([len(sentence.split()) for sentence in pos_segs])))\n",
    "print(\"Min length: {}\".format(np.min([len(sentence.split()) for sentence in pos_segs])))\n",
    "pos_text_seg = \" \".join(pos_segs)\n",
    "c = Counter(pos_text_seg.split()).most_common(100)\n",
    "print(\"Most common words : \\n{} \\n\".format(c))\n",
    "\n",
    "print(\"-\" * 20 + \" NEGATIVE TEXT \" + \"-\" * 20)\n",
    "print(\"Total number: {}\".format(len(neg_segs)))\n",
    "print(\"Average length: {}\".format(np.mean([len(sentence.split()) for sentence in neg_segs])))\n",
    "print(\"Max length: {}\".format(np.max([len(sentence.split()) for sentence in neg_segs])))\n",
    "print(\"Min length: {}\".format(np.min([len(sentence.split()) for sentence in neg_segs])))\n",
    "neg_text_seg = \" \".join(neg_segs)\n",
    "c = Counter(neg_text_seg.split()).most_common(100)\n",
    "print(\"Most common words : \\n{} \\n\".format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正样本数量从25000变成了24967，负样本数量从25000变成了24964."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本的平均长度为17，我们取20作为句子长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LIMIT_SIZE = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造词典\n",
    "由于语料中还含有很多低频词，在构建词典时需要将这些词过滤掉，这样不仅可以加快模型的执行效率，还能减少特殊词带来的噪声干扰。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('数选', 1),\n",
       " ('服务文明', 1),\n",
       " ('墨绿色', 1),\n",
       " ('买该', 1),\n",
       " ('第一波', 1),\n",
       " ('救灾', 1),\n",
       " ('手起刀落', 1),\n",
       " ('资量', 1),\n",
       " ('适服', 1),\n",
       " ('挺酷', 1),\n",
       " ('不太显', 1),\n",
       " ('自找麻烦', 1),\n",
       " ('亲快', 1),\n",
       " ('何乐不为', 1),\n",
       " ('一以', 1),\n",
       " ('超有', 1),\n",
       " ('这价能', 1),\n",
       " ('浸水', 1),\n",
       " ('版式', 1),\n",
       " ('赞得', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter(' '.join(pos_segs + neg_segs).split())\n",
    "sorted(c.most_common(), key=lambda x: x[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of vocabulary is: 22831\n"
     ]
    }
   ],
   "source": [
    "# 初始化两个token：pad和unk\n",
    "vocab = [\"<pad>\", \"<unk>\"]\n",
    "\n",
    "# 去除出现频次为1次的单词\n",
    "for w, f in c.most_common():\n",
    "    if f > 1:\n",
    "        vocab.append(w)\n",
    "print(\"Total size of vocabulary is: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造映射\n",
    "构造将单词转换为编码和将编码转为单词的映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_token = {word: token for token, word in enumerate(vocab)}\n",
    "token_to_word = {token: word for word, token in word_to_token.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本转token\n",
    "def convert_text_to_token(sentence, word_to_token_map=word_to_token, limit_size=SENTENCE_LIMIT_SIZE):\n",
    "    # 获取unknown单词和pad的token\n",
    "    unk_id = word_to_token_map[\"<unk>\"]\n",
    "    pad_id = word_to_token_map[\"<pad>\"]\n",
    "    \n",
    "    # 对句子进行token转换，对于未在词典中出现过的词用unk的token填充\n",
    "    tokens = [word_to_token_map.get(word, unk_id) for word in sentence.lower().split()]\n",
    "    \n",
    "    # Pad\n",
    "    if len(tokens) < limit_size:\n",
    "        tokens.extend([0] * (limit_size - len(tokens)))\n",
    "    # Trunc\n",
    "    else:\n",
    "        tokens = tokens[:limit_size]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24967/24967 [00:00<00:00, 156588.04it/s]\n"
     ]
    }
   ],
   "source": [
    "pos_tokens = []\n",
    "\n",
    "for sentence in tqdm.tqdm(pos_segs):\n",
    "    tokens = convert_text_to_token(sentence)\n",
    "    pos_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15, 210, 3, 337, 241, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [24,\n",
       "  15,\n",
       "  2526,\n",
       "  3,\n",
       "  37,\n",
       "  93,\n",
       "  2718,\n",
       "  4524,\n",
       "  6424,\n",
       "  112,\n",
       "  27,\n",
       "  495,\n",
       "  1914,\n",
       "  77,\n",
       "  56,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [210, 2527, 15, 3, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24964/24964 [00:00<00:00, 103385.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[37,\n",
       "  45,\n",
       "  135,\n",
       "  1342,\n",
       "  180,\n",
       "  2034,\n",
       "  152,\n",
       "  144,\n",
       "  1327,\n",
       "  56,\n",
       "  372,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [224, 59, 51, 716, 2, 1611, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [34,\n",
       "  2421,\n",
       "  2891,\n",
       "  11024,\n",
       "  588,\n",
       "  11786,\n",
       "  696,\n",
       "  80,\n",
       "  15,\n",
       "  164,\n",
       "  1136,\n",
       "  157,\n",
       "  542,\n",
       "  45,\n",
       "  80,\n",
       "  37,\n",
       "  93,\n",
       "  3238,\n",
       "  20479,\n",
       "  304]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_tokens = []\n",
    "\n",
    "for sentence in tqdm.tqdm(neg_segs):\n",
    "    tokens = convert_text_to_token(sentence)\n",
    "    neg_tokens.append(tokens)\n",
    "    \n",
    "neg_tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all tokens: (49931, 20)\n"
     ]
    }
   ],
   "source": [
    "# 合并语料\n",
    "pos_tokens = np.array(pos_tokens)\n",
    "neg_tokens = np.array(neg_tokens)\n",
    "total_tokens = np.concatenate((pos_tokens, neg_tokens), axis=0)\n",
    "print(\"Shape of all tokens: ({}, {})\".format(*total_tokens.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all labels: (49931, 20)\n"
     ]
    }
   ],
   "source": [
    "# 合并类标\n",
    "pos_targets = np.ones((pos_tokens.shape[0]))\n",
    "neg_targets = np.zeros((neg_tokens.shape[0]))\n",
    "total_targets = np.concatenate((pos_targets, neg_targets), axis=0).reshape(-1, 1)\n",
    "print(\"Shape of all labels: ({}, {})\".format(*total_tokens.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练word2vec词向量\n",
    "with open(\"/Users/huih/Downloads/sgns.weibo.bigram-char\", 'r') as f:\n",
    "    words = set()\n",
    "    word_to_vec = {}\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        # 当前单词\n",
    "        curr_word = line[0]\n",
    "        words.add(curr_word)\n",
    "        # 当前词向量\n",
    "        word_to_vec[curr_word] = np.array(line[1:], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words which have pretrained-vectors in vocab is: 17904\n",
      "\n",
      "The number of words which do not have pretrained-vectors in vocab is : 4927\n"
     ]
    }
   ],
   "source": [
    "len(words)\n",
    "print(\"The number of words which have pretrained-vectors in vocab is: {}\".format(len(set(vocab)&set(words))))\n",
    "print()\n",
    "print(\"The number of words which do not have pretrained-vectors in vocab is : {}\".format(len(set(vocab))-\n",
    "                                                                                         len(set(vocab)&set(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22831"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in embedding : 17904\n",
      "Number of words not in embedding : 4927\n"
     ]
    }
   ],
   "source": [
    "# 在预训练词向量中的词\n",
    "wordSet = set(vocab) & set(words)\n",
    "print(\"Number of words in embedding : {}\".format(len(wordSet)))\n",
    "print(\"Number of words not in embedding : {}\".format(len(set(vocab)) - len(wordSet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造词向量矩阵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22831/22831 [00:00<00:00, 55596.03it/s]\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_SIZE = 300\n",
    "\n",
    "static_embeddings = np.zeros([VOCAB_SIZE, EMBEDDING_SIZE])\n",
    "\n",
    "for word, token in tqdm.tqdm(word_to_token.items()):\n",
    "    # 用词向量填充，如果没有对应的词向量，则用随机数填充\n",
    "    word_vector = word_to_vec.get(word, 0.2 * np.random.random(EMBEDDING_SIZE) - 0.1)\n",
    "    static_embeddings[token, :] = word_vector\n",
    "\n",
    "# 重置PAD为0向量\n",
    "pad_id = word_to_token[\"<pad>\"]\n",
    "static_embeddings[pad_id, :] = np.zeros(EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_embeddings = static_embeddings.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(total_tokens, total_targets, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练集进一步划分为训练和验证集，将测试集留作模型测试使用\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:31955, val:7989, test:9987\n"
     ]
    }
   ],
   "source": [
    "print(\"train:{}, val:{}, test:{}\".format(len(y_train), len(y_val), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "def get_batch(x, y, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    assert x.shape[0] == y.shape[0], print(\"error shape!\")\n",
    "    # shuffle\n",
    "    if shuffle:\n",
    "        shuffled_index = np.random.permutation(range(x.shape[0]))\n",
    "\n",
    "        x = x[shuffled_index]\n",
    "        y = y[shuffled_index]\n",
    "    \n",
    "    # 统计共几个完整的batch\n",
    "    n_batches = int(x.shape[0] / batch_size)\n",
    "    \n",
    "    for i in range(n_batches - 1):\n",
    "        x_batch = x[i*batch_size: (i+1)*batch_size]\n",
    "        y_batch = y[i*batch_size: (i+1)*batch_size]\n",
    "    \n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清空图\n",
    "tf.reset_default_graph()\n",
    "\n",
    "filters_size = [2, 3, 4, 5]\n",
    "num_filters = 100\n",
    "# 超参数\n",
    "BATCH_SIZE = 256\n",
    "EPOCHES = 50\n",
    "LEARNING_RATE = 0.003\n",
    "L2_LAMBDA = 10\n",
    "KEEP_PROB = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"cnn\"):\n",
    "    with tf.name_scope(\"placeholders\"):\n",
    "        inputs = tf.placeholder(dtype=tf.int32, shape=(None, 20), name=\"inputs\")\n",
    "        targets = tf.placeholder(dtype=tf.float32, shape=(None, 1), name=\"targets\")\n",
    "    \n",
    "    # embeddings\n",
    "    with tf.name_scope(\"embeddings\"):\n",
    "        embedding_matrix = tf.Variable(initial_value=static_embeddings, trainable=False, name=\"embedding_matrix\")\n",
    "        embed = tf.nn.embedding_lookup(embedding_matrix, inputs, name=\"embed\")\n",
    "        # 添加channel维度\n",
    "        embed_expanded = tf.expand_dims(embed, -1, name=\"embed_expand\")\n",
    "    \n",
    "    # 用来存储max-pooling的结果\n",
    "    pooled_outputs = []\n",
    "\n",
    "    # 迭代多个filter\n",
    "    for i, filter_size in enumerate(filters_size):\n",
    "        with tf.name_scope(\"conv_maxpool_%s\" % filter_size):\n",
    "            filter_shape = [filter_size, EMBEDDING_SIZE, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, mean=0.0, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.zeros(num_filters), name=\"b\")\n",
    "\n",
    "            conv = tf.nn.conv2d(input=embed_expanded, \n",
    "                                 filter=W, \n",
    "                                 strides=[1, 1, 1, 1], \n",
    "                                 padding=\"VALID\",\n",
    "                                 name=\"conv\")\n",
    "\n",
    "            # 激活\n",
    "            a = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"activations\")\n",
    "            # 池化\n",
    "            max_pooling = tf.nn.max_pool(value=a, \n",
    "                                    ksize=[1, SENTENCE_LIMIT_SIZE - filter_size + 1, 1, 1],\n",
    "                                    strides=[1, 1, 1, 1],\n",
    "                                    padding=\"VALID\",\n",
    "                                    name=\"max_pooling\")\n",
    "            pooled_outputs.append(max_pooling)\n",
    "    \n",
    "    # 统计所有的filter\n",
    "    total_filters = num_filters * len(filters_size)\n",
    "    total_pool = tf.concat(pooled_outputs, 3)\n",
    "    flattend_pool = tf.reshape(total_pool, (-1, total_filters))\n",
    "    \n",
    "    # dropout\n",
    "    with tf.name_scope(\"dropout\"):\n",
    "        dropout = tf.nn.dropout(flattend_pool, KEEP_PROB)\n",
    "    \n",
    "    # output\n",
    "    with tf.name_scope(\"output\"):\n",
    "        W = tf.get_variable(\"W\", shape=(total_filters, 1), initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.Variable(tf.zeros(1), name=\"b\")\n",
    "        \n",
    "        logits = tf.add(tf.matmul(dropout, W), b)\n",
    "        predictions = tf.nn.sigmoid(logits, name=\"predictions\")\n",
    "    \n",
    "    # loss\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=targets, logits=logits))\n",
    "        loss = loss + L2_LAMBDA * tf.nn.l2_loss(W)\n",
    "        optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "    # evaluation\n",
    "    with tf.name_scope(\"evaluation\"):\n",
    "        correct_preds = tf.equal(tf.cast(tf.greater(predictions, 0.5), tf.float32), targets)\n",
    "        accuracy = tf.reduce_sum(tf.reduce_sum(tf.cast(correct_preds, tf.float32), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_train_accuracy = []\n",
    "cnn_test_accuracy = []\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "TRAIN_FLAG = False\n",
    "\n",
    "if TRAIN_FLAG:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        writer = tf.summary.FileWriter(\"./graphs/cnn\", tf.get_default_graph())\n",
    "        n_batches = int(x_train.shape[0] / BATCH_SIZE)\n",
    "\n",
    "        for epoch in range(EPOCHES):\n",
    "            total_loss = 0\n",
    "            for x_batch, y_batch in get_batch(x_train, y_train):\n",
    "                _, l = sess.run([optimizer, loss],\n",
    "                                feed_dict={inputs: x_batch, \n",
    "                                           targets: y_batch})\n",
    "                total_loss += l\n",
    "\n",
    "            train_corrects = sess.run(accuracy, feed_dict={inputs: x_train, targets: y_train})\n",
    "            train_acc = train_corrects / x_train.shape[0]\n",
    "            cnn_train_accuracy.append(train_acc)\n",
    "\n",
    "            val_corrects = sess.run(accuracy, feed_dict={inputs: x_val, targets: y_val})\n",
    "            val_acc = val_corrects / x_val.shape[0]\n",
    "            cnn_test_accuracy.append(val_acc)\n",
    "\n",
    "            print(\"Training epoch: {}, Training loss: {:.4f}, Train accuracy: {:.4f}, Val accuracy: {:.4f}\".format(epoch + 1, \n",
    "                                                                                                                    total_loss / n_batches,\n",
    "                                                                                                                    train_acc,\n",
    "                                                                                                                    val_acc))\n",
    "\n",
    "        saver.save(sess, \"checkpoints/cnn\")\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a43d2d4a8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnn_train_accuracy)\n",
    "plt.plot(cnn_test_accuracy)\n",
    "plt.ylim(ymin=0.5, ymax=1.01)\n",
    "plt.title(\"The accuracy of CNN model\")\n",
    "plt.legend([\"train\", \"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-baf6d93cd55d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"checkpoints/cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     total_correct = sess.run(accuracy,\n\u001b[1;32m      5\u001b[0m                              feed_dict={inputs: x_test, targets: y_test})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"checkpoints/cnn\")\n",
    "    \n",
    "    total_correct = sess.run(accuracy,\n",
    "                             feed_dict={inputs: x_test, targets: y_test})\n",
    "\n",
    "    print(\"The CNN model accuracy on test set: {:.2f}%\".format(100 * total_correct / x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
